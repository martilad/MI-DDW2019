{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MI-DDW - Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/users/m/martilad/mi-ddw/t2/__venv__/lib/python3.7/site-packages (3.4)\n",
      "Requirement already satisfied: six in /home/users/m/martilad/mi-ddw/t2/__venv__/lib/python3.7/site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: singledispatch in /home/users/m/martilad/mi-ddw/t2/__venv__/lib/python3.7/site-packages (from nltk) (3.4.0.3)\n",
      "Requirement already satisfied: numpy in /home/users/m/martilad/mi-ddw/t2/__venv__/lib/python3.7/site-packages (1.16.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk\n",
    "! pip install numpy\n",
    "# ! pip install twython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /home/users/m/martilad/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     /home/users/m/martilad/nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /home/users/m/martilad/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/users/m/martilad/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/users/m/martilad/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/users/m/martilad/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/users/m/martilad/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/users/m/martilad/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/users/m/martilad/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /home/users/m/martilad/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download()\n",
    "# Download Corpora -> brown webtext words stopwords\n",
    "# Download Models -> punkt averaged_perceptron_tagger maxent_ne_chunker vader_lexicon wordnet tagsets\n",
    "nltk.download([\"brown\",\"webtext\", \"words\", \"stopwords\"] )\n",
    "nltk.download([\"punkt\", \"averaged_perceptron_tagger\", \"maxent_ne_chunker\", \"vader_lexicon\", \"wordnet\", \"tagsets\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.words()[0:10]\n",
    "brown.tagged_words()[0:10]\n",
    "len(brown.words())\n",
    "# dir(brown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text.txt', 'r') as myfile:\n",
    "    text = myfile.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 9132),\n",
       " ('to', 4086),\n",
       " ('.', 4085),\n",
       " ('the', 4058),\n",
       " ('of', 3596),\n",
       " ('and', 3423),\n",
       " ('her', 2107),\n",
       " ('I', 2065),\n",
       " ('a', 1897),\n",
       " ('was', 1837)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "tokens = nltk.word_tokenize(text)\n",
    "def tokenCounts(tokens):\n",
    "    counts = Counter(tokens)\n",
    "    sortedCounts = sorted(counts.items(), key=lambda count:count[1], reverse=True)\n",
    "    return sortedCounts\n",
    "\n",
    "tokenCounts(tokens)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 4086),\n",
       " ('the', 4058),\n",
       " ('of', 3596),\n",
       " ('and', 3423),\n",
       " ('her', 2107),\n",
       " ('I', 2065),\n",
       " ('a', 1897),\n",
       " ('was', 1837),\n",
       " ('in', 1779),\n",
       " ('not', 1496)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "punctuation += '“”--'\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "nopunc_tokens = [token for token in tokens if token not in punctuation]\n",
    "tokenCounts(nopunc_tokens)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 2065),\n",
       " ('Mr.', 768),\n",
       " ('Elizabeth', 631),\n",
       " (\"'s\", 574),\n",
       " ('could', 513),\n",
       " ('would', 465),\n",
       " ('said', 401),\n",
       " ('Darcy', 401),\n",
       " ('Mrs.', 337),\n",
       " ('She', 324)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words('english')\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "filtered_tokens = [token for token in tokens if token not in punctuation]\n",
    "filtered_tokens = [token for token in filtered_tokens if token not in stops]\n",
    "tokenCounts(filtered_tokens)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“Oh!', 'She is the most beautiful creature I ever beheld!', 'But there is one\\nof her sisters sitting down just behind you, who is very pretty, and I\\ndare say very agreeable.', 'Do let me ask my partner to introduce you.”\\n\\n“Which do you mean?” and turning round he looked for a moment at\\nElizabeth, till catching her eye, he withdrew his own and coldly said:\\n“She is tolerable, but not handsome enough to tempt _me_; I am in no\\nhumour at present to give consequence to young ladies who are slighted\\nby other men.', 'You had better return to your partner and enjoy her\\nsmiles, for you are wasting your time with me.”\\n\\nMr. Bingley followed his advice.']\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "print(sentences[100:105])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['“', 'Oh', '!'], ['She', 'is', 'the', 'most', 'beautiful', 'creature', 'I', 'ever', 'beheld', '!'], ['But', 'there', 'is', 'one', 'of', 'her', 'sisters', 'sitting', 'down', 'just', 'behind', 'you', ',', 'who', 'is', 'very', 'pretty', ',', 'and', 'I', 'dare', 'say', 'very', 'agreeable', '.'], ['Do', 'let', 'me', 'ask', 'my', 'partner', 'to', 'introduce', 'you.', '”', '“', 'Which', 'do', 'you', 'mean', '?', '”', 'and', 'turning', 'round', 'he', 'looked', 'for', 'a', 'moment', 'at', 'Elizabeth', ',', 'till', 'catching', 'her', 'eye', ',', 'he', 'withdrew', 'his', 'own', 'and', 'coldly', 'said', ':', '“', 'She', 'is', 'tolerable', ',', 'but', 'not', 'handsome', 'enough', 'to', 'tempt', '_me_', ';', 'I', 'am', 'in', 'no', 'humour', 'at', 'present', 'to', 'give', 'consequence', 'to', 'young', 'ladies', 'who', 'are', 'slighted', 'by', 'other', 'men', '.'], ['You', 'had', 'better', 'return', 'to', 'your', 'partner', 'and', 'enjoy', 'her', 'smiles', ',', 'for', 'you', 'are', 'wasting', 'your', 'time', 'with', 'me.', '”', 'Mr.', 'Bingley', 'followed', 'his', 'advice', '.']]\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(text)\n",
    "tokens = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "print(tokens[100:105])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part-Of-Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(text)\n",
    "tokens = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "tagged = [nltk.pos_tag(sent) for sent in tokens]\n",
    "\n",
    "print(tagged[100:105])\n",
    "verbs = 0\n",
    "nouns = 0\n",
    "for i in tagged:\n",
    "    for j in i:\n",
    "        \n",
    "        if j[1] == 'V':\n",
    "            verbs+=1\n",
    "        if j[1] == 'N':\n",
    "            noun+=1\n",
    "print('verbs', verbs)\n",
    "print('noun', nouns)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('JJ*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "univers\n",
      "remors\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "stems = {token:stemmer.stem(token) for token in tokens}\n",
    "#print(stems)\n",
    "print(stems['universally'])\n",
    "print(stems['remorse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "universally\n",
      "remorse\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "lemmas = {token:lemmatizer.lemmatize(token) for token in tokens}\n",
    "print(lemmas['universally'])\n",
    "print(lemmas['remorse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('you', 'PRP'), ('heard', 'VBP'), ('that', 'IN'), Tree('NE', [('Netherfield', 'NNP'), ('Park', 'NNP')]), ('is', 'VBZ')]\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(text)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "ne_chunked = nltk.ne_chunk(tagged, binary=True)\n",
    "print(ne_chunked[100:105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Jane Austen': 'PERSON',\n",
       " 'Mr. Bennet': 'PERSON',\n",
       " 'Netherfield Park': 'PERSON',\n",
       " 'Long': 'PERSON',\n",
       " 'Netherfield': 'ORGANIZATION',\n",
       " 'England': 'GPE',\n",
       " 'Mr. Morris': 'PERSON',\n",
       " 'Michaelmas': 'ORGANIZATION',\n",
       " 'Single': 'PERSON',\n",
       " 'Mr. Bingley': 'PERSON',\n",
       " 'Sir William': 'PERSON',\n",
       " 'Lady Lucas': 'PERSON',\n",
       " 'Lizzy': 'PERSON',\n",
       " 'Jane': 'PERSON',\n",
       " 'Lydia': 'PERSON',\n",
       " 'Bennet': 'PERSON',\n",
       " 'least.': 'ORGANIZATION',\n",
       " 'Elizabeth': 'PERSON',\n",
       " 'Kitty': 'PERSON',\n",
       " 'Heaven': 'PERSON',\n",
       " 'Mary': 'PERSON',\n",
       " 'Mr. Bingley.': 'PERSON',\n",
       " 'Hertfordshire': 'ORGANIZATION',\n",
       " 'London': 'GPE',\n",
       " 'Mr. Hurst': 'PERSON',\n",
       " 'Mr. Darcy': 'PERSON',\n",
       " 'Derbyshire': 'ORGANIZATION',\n",
       " 'Miss Bingley': 'PERSON',\n",
       " 'Elizabeth Bennet': 'PERSON',\n",
       " 'Come': 'ORGANIZATION',\n",
       " 'Darcy': 'PERSON',\n",
       " 'Catherine': 'ORGANIZATION',\n",
       " 'Longbourn': 'ORGANIZATION',\n",
       " 'Everybody': 'PERSON',\n",
       " 'Miss Lucas': 'PERSON',\n",
       " 'Miss King': 'PERSON',\n",
       " 'Maria Lucas': 'PERSON',\n",
       " 'God': 'PERSON',\n",
       " 'Netherfield House': 'FACILITY',\n",
       " 'Bingley': 'PERSON',\n",
       " 'Meryton': 'GPE',\n",
       " 'Miss Bennet': 'PERSON',\n",
       " 'Bennets': 'ORGANIZATION',\n",
       " 'Sir William Lucas': 'PERSON',\n",
       " 'Lucas Lodge': 'PERSON',\n",
       " 'St. James': 'ORGANIZATION',\n",
       " 'Miss Lucases': 'ORGANIZATION',\n",
       " 'Miss Bennets': 'PERSON',\n",
       " '_You_': 'PERSON',\n",
       " 'Charlotte': 'PERSON',\n",
       " 'Mr.': 'PERSON',\n",
       " 'Mr. Robinson': 'PERSON',\n",
       " 'Eliza': 'PERSON',\n",
       " 'Vanity': 'PERSON',\n",
       " 'Lucas': 'PERSON',\n",
       " 'Commerce': 'ORGANIZATION',\n",
       " 'Sir': 'GPE',\n",
       " 'William Lucas': 'PERSON',\n",
       " 'Colonel Forster': 'ORGANIZATION',\n",
       " 'Colonel': 'ORGANIZATION',\n",
       " 'Scotch': 'GPE',\n",
       " 'Irish': 'GPE',\n",
       " 'Lucases': 'ORGANIZATION',\n",
       " 'William': 'PERSON',\n",
       " 'Lady': 'PERSON',\n",
       " 'Miss Eliza': 'PERSON',\n",
       " 'Miss Elizabeth': 'ORGANIZATION',\n",
       " 'Pemberley': 'ORGANIZATION',\n",
       " 'Mr. Phillips': 'PERSON',\n",
       " 'Captain Carter': 'PERSON',\n",
       " 'Miss Watson': 'PERSON',\n",
       " 'Clarke': 'GPE',\n",
       " 'MY': 'ORGANIZATION',\n",
       " 'Louisa': 'PERSON',\n",
       " 'BINGLEY': 'ORGANIZATION',\n",
       " 'Breakfast': 'PERSON',\n",
       " 'Mr. Jones': 'PERSON',\n",
       " 'Captain': 'GPE',\n",
       " 'Miss Elizabeth Bennet': 'PERSON',\n",
       " 'Miss Jane Bennet': 'PERSON',\n",
       " 'Eliza Bennet': 'PERSON',\n",
       " 'Charles': 'PERSON',\n",
       " 'Caroline': 'PERSON',\n",
       " 'Miss Darcy': 'PERSON',\n",
       " 'Madam': 'PERSON',\n",
       " 'Mamma': 'PERSON',\n",
       " 'Charlotte Lucas': 'PERSON',\n",
       " '_That_': 'PERSON',\n",
       " 'Gardiner': 'PERSON',\n",
       " 'Carter': 'PERSON',\n",
       " 'Hurst': 'PERSON',\n",
       " 'How': 'PERSON',\n",
       " 'Tell': 'ORGANIZATION',\n",
       " 'Miss': 'PERSON',\n",
       " 'Nay': 'PERSON',\n",
       " 'Italian': 'GPE',\n",
       " 'Phillips': 'PERSON',\n",
       " 'Nicholls': 'PERSON',\n",
       " 'Miss Eliza Bennet': 'ORGANIZATION',\n",
       " 'Intimate': 'ORGANIZATION',\n",
       " 'Tease': 'PERSON',\n",
       " 'Implacable': 'ORGANIZATION',\n",
       " 'Steady': 'PERSON',\n",
       " 'Much': 'PERSON',\n",
       " 'Mr. Collins': 'PERSON',\n",
       " 'Westerham': 'GPE',\n",
       " 'Kent': 'GPE',\n",
       " 'Easter': 'PERSON',\n",
       " 'Right Honourable Lady Catherine': 'ORGANIZATION',\n",
       " 'Bourgh': 'GPE',\n",
       " 'Sir Lewis de Bourgh': 'PERSON',\n",
       " 'Church': 'ORGANIZATION',\n",
       " 'Lady Catherine': 'PERSON',\n",
       " 'COLLINS': 'ORGANIZATION',\n",
       " 'No': 'ORGANIZATION',\n",
       " 'Rosings': 'GPE',\n",
       " 'Rosings Park': 'GPE',\n",
       " 'Her': 'PERSON',\n",
       " 'British': 'GPE',\n",
       " 'Fordyce': 'PERSON',\n",
       " 'Richard': 'PERSON',\n",
       " 'Mr. Denny': 'PERSON',\n",
       " 'Hunsford': 'GPE',\n",
       " 'Mr. Wickham': 'PERSON',\n",
       " 'Mr. Phillip': 'PERSON',\n",
       " 'Wickham': 'PERSON',\n",
       " 'Denny': 'PERSON',\n",
       " 'Society': 'PERSON',\n",
       " 'Family': 'PERSON',\n",
       " 'Pemberley House': 'PERSON',\n",
       " 'Lady Anne Darcy': 'PERSON',\n",
       " 'Vain': 'PERSON',\n",
       " 'Collins': 'PERSON',\n",
       " 'Longbourn House': 'FACILITY',\n",
       " 'Archbishop': 'ORGANIZATION',\n",
       " 'Hunsford Parsonage': 'ORGANIZATION',\n",
       " 'Bingleys': 'ORGANIZATION',\n",
       " 'first.': 'ORGANIZATION',\n",
       " 'George Wickham': 'PERSON',\n",
       " 'Meryton.': 'ORGANIZATION',\n",
       " 'Miss de Bourgh': 'PERSON',\n",
       " 'Jenkinson': 'PERSON',\n",
       " 'Choose': 'PERSON',\n",
       " 'Aye': 'PERSON',\n",
       " 'York': 'ORGANIZATION',\n",
       " 'Miss Lizzy': 'PERSON',\n",
       " 'Caroline Bingley': 'PERSON',\n",
       " 'Grosvenor Street': 'GPE',\n",
       " 'Georgiana Darcy': 'PERSON',\n",
       " 'Such': 'PERSON',\n",
       " 'Good Lord': 'PERSON',\n",
       " 'Between Elizabeth': 'PERSON',\n",
       " 'Hope': 'PERSON',\n",
       " 'Thank God': 'PERSON',\n",
       " 'Christmas': 'ORGANIZATION',\n",
       " 'Mr. Gardiner': 'PERSON',\n",
       " 'Pray': 'PERSON',\n",
       " 'Poor Jane': 'PERSON',\n",
       " 'Gracechurch Street': 'FACILITY',\n",
       " 'Gardiners': 'ORGANIZATION',\n",
       " 'Phillipses': 'ORGANIZATION',\n",
       " 'Mr. Fitzwilliam Darcy': 'PERSON',\n",
       " 'Maria': 'GPE',\n",
       " 'March': 'PERSON',\n",
       " 'Grosvenor': 'GPE',\n",
       " 'Absence': 'PERSON',\n",
       " 'Gracechurch': 'GPE',\n",
       " 'Thank Heaven': 'PERSON',\n",
       " 'Adieu': 'PERSON',\n",
       " 'Lakes': 'GPE',\n",
       " 'Parsonage': 'ORGANIZATION',\n",
       " 'Make': 'PERSON',\n",
       " 'Sir Lewis': 'PERSON',\n",
       " 'Lewis de Bourgh': 'PERSON',\n",
       " 'Miss Pope': 'PERSON',\n",
       " 'Hunsford Lane': 'ORGANIZATION',\n",
       " 'Park': 'ORGANIZATION',\n",
       " 'Colonel Fitzwilliam': 'ORGANIZATION',\n",
       " 'Fitzwilliam': 'PERSON',\n",
       " 'Georgiana': 'PERSON',\n",
       " 'Anne': 'PERSON',\n",
       " 'Mrs. Collins': 'PERSON',\n",
       " 'Longbourn.': 'ORGANIZATION',\n",
       " 'Did Mr. Darcy': 'PERSON',\n",
       " 'Wilfully': 'PERSON',\n",
       " 'Cambridge': 'ORGANIZATION',\n",
       " 'Ramsgate': 'ORGANIZATION',\n",
       " 'Regard': 'PERSON',\n",
       " 'FITZWILLIAM': 'ORGANIZATION',\n",
       " 'Militia': 'PERSON',\n",
       " 'Dawson': 'PERSON',\n",
       " 'Lady Anne': 'PERSON',\n",
       " 'John': 'PERSON',\n",
       " 'Bromley': 'PERSON',\n",
       " 'Bell': 'ORGANIZATION',\n",
       " 'Anxiety': 'PERSON',\n",
       " 'Rosings.': 'ORGANIZATION',\n",
       " 'Poor Charlotte': 'PERSON',\n",
       " 'Good': 'ORGANIZATION',\n",
       " 'Brighton': 'GPE',\n",
       " 'Mary King': 'PERSON',\n",
       " 'Liverpool': 'ORGANIZATION',\n",
       " 'Lord': 'PERSON',\n",
       " 'Harriet': 'PERSON',\n",
       " 'Pen': 'PERSON',\n",
       " 'Pratt': 'GPE',\n",
       " 'George': 'GPE',\n",
       " 'Dear Lizzy': 'PERSON',\n",
       " 'Collinses': 'ORGANIZATION',\n",
       " '_They_': 'PERSON',\n",
       " 'Good Heaven': 'PERSON',\n",
       " 'Colonel Miller': 'PERSON',\n",
       " 'Had Lydia': 'PERSON',\n",
       " 'Respect': 'PERSON',\n",
       " 'War Office': 'FACILITY',\n",
       " 'July': 'GPE',\n",
       " 'Matlock': 'GPE',\n",
       " 'Chatsworth': 'PERSON',\n",
       " 'Dovedale': 'GPE',\n",
       " 'Peak': 'ORGANIZATION',\n",
       " 'Oxford': 'GPE',\n",
       " 'Blenheim': 'GPE',\n",
       " 'Warwick': 'PERSON',\n",
       " 'Kenilworth': 'PERSON',\n",
       " 'Birmingham': 'GPE',\n",
       " 'Lambton': 'PERSON',\n",
       " 'Pemberley Woods': 'PERSON',\n",
       " 'Reynolds': 'PERSON',\n",
       " 'Bakewell': 'ORGANIZATION',\n",
       " 'Dove Dale': 'PERSON',\n",
       " 'Netherfield.': 'ORGANIZATION',\n",
       " 'Spanish': 'GPE',\n",
       " 'Annesley': 'PERSON',\n",
       " 'Scotland': 'GPE',\n",
       " 'Dearest Lizzy': 'PERSON',\n",
       " 'Gretna Green': 'GPE',\n",
       " 'Clapham': 'PERSON',\n",
       " 'Epsom': 'PERSON',\n",
       " 'Barnet': 'GPE',\n",
       " 'Hatfield': 'PERSON',\n",
       " 'Colonel F.': 'PERSON',\n",
       " 'Poor Kitty': 'PERSON',\n",
       " 'Surprise': 'PERSON',\n",
       " 'Had Elizabeth': 'PERSON',\n",
       " 'Forsters': 'ORGANIZATION',\n",
       " 'Poor': 'PERSON',\n",
       " 'Could Colonel Forster': 'PERSON',\n",
       " 'LYDIA': 'ORGANIZATION',\n",
       " 'Rendered': 'PERSON',\n",
       " 'Eastbourne': 'ORGANIZATION',\n",
       " 'Balls': 'PERSON',\n",
       " 'Hill': 'PERSON',\n",
       " 'Haggerston': 'PERSON',\n",
       " 'Stay': 'PERSON',\n",
       " 'Girls': 'PERSON',\n",
       " 'Miss Lydia': 'PERSON',\n",
       " 'Poor Lydia': 'PERSON',\n",
       " 'Great': 'GPE',\n",
       " 'Miss Lydia Bennet': 'PERSON',\n",
       " 'Haye Park': 'PERSON',\n",
       " 'Gouldings': 'ORGANIZATION',\n",
       " 'Stoke': 'FACILITY',\n",
       " 'Ashworth': 'PERSON',\n",
       " 'Pulvis Lodge': 'PERSON',\n",
       " 'Into': 'ORGANIZATION',\n",
       " \"Mr. Bennet's\": 'PERSON',\n",
       " 'General': 'GPE',\n",
       " 'North': 'LOCATION',\n",
       " 'South': 'LOCATION',\n",
       " 'Smiles': 'PERSON',\n",
       " 'William Goulding': 'PERSON',\n",
       " 'Newcastle': 'GPE',\n",
       " 'Are': 'PERSON',\n",
       " 'St. Clement': 'ORGANIZATION',\n",
       " 'Little': 'ORGANIZATION',\n",
       " 'Mr. Stone': 'PERSON',\n",
       " '_I_': 'PERSON',\n",
       " '_He_': 'PERSON',\n",
       " 'Poor Reynolds': 'ORGANIZATION',\n",
       " 'Kympton': 'GPE',\n",
       " 'Parsonage House': 'ORGANIZATION',\n",
       " 'Happy': 'PERSON',\n",
       " 'Times': 'ORGANIZATION',\n",
       " 'Esq': 'PERSON',\n",
       " 'Anxious': 'PERSON',\n",
       " 'Scarborough': 'GPE',\n",
       " 'Well': 'ORGANIZATION',\n",
       " 'French': 'GPE',\n",
       " 'Sarah': 'PERSON',\n",
       " 'Where': 'PERSON',\n",
       " 'Darcy_': 'PERSON',\n",
       " 'Never': 'PERSON',\n",
       " 'Christian': 'GPE',\n",
       " 'Pemberley.': 'ORGANIZATION',\n",
       " 'Oakham Mount': 'PERSON',\n",
       " 'Mount': 'GPE',\n",
       " 'Yours': 'PERSON',\n",
       " 'DEAR': 'ORGANIZATION',\n",
       " 'Console Lady Catherine': 'PERSON',\n",
       " 'Bath': 'GPE'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extractEntities(ne_chunked):\n",
    "    data = {}\n",
    "    for entity in ne_chunked:\n",
    "        if isinstance(entity, nltk.tree.Tree):\n",
    "            text = \" \".join([word for word, tag in entity.leaves()])\n",
    "            ent = entity.label()\n",
    "            data[text] = ent\n",
    "        else:\n",
    "            continue\n",
    "    return data\n",
    "\n",
    "extractEntities(ne_chunked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Jane Austen': 'PERSON',\n",
       " 'Mr. Bennet': 'PERSON',\n",
       " 'Netherfield Park': 'PERSON',\n",
       " 'Long': 'PERSON',\n",
       " 'Netherfield': 'ORGANIZATION',\n",
       " 'England': 'GPE',\n",
       " 'Mr. Morris': 'PERSON',\n",
       " 'Michaelmas': 'ORGANIZATION',\n",
       " 'Single': 'PERSON',\n",
       " 'Mr. Bingley': 'PERSON',\n",
       " 'Sir William': 'PERSON',\n",
       " 'Lady Lucas': 'PERSON',\n",
       " 'Lizzy': 'PERSON',\n",
       " 'Jane': 'PERSON',\n",
       " 'Lydia': 'PERSON',\n",
       " 'Bennet': 'PERSON',\n",
       " 'least.': 'ORGANIZATION',\n",
       " 'Elizabeth': 'PERSON',\n",
       " 'Kitty': 'PERSON',\n",
       " 'Heaven': 'PERSON',\n",
       " 'Mary': 'PERSON',\n",
       " 'Mr. Bingley.': 'PERSON',\n",
       " 'Hertfordshire': 'ORGANIZATION',\n",
       " 'London': 'GPE',\n",
       " 'Mr. Hurst': 'PERSON',\n",
       " 'Mr. Darcy': 'PERSON',\n",
       " 'Derbyshire': 'ORGANIZATION',\n",
       " 'Miss Bingley': 'PERSON',\n",
       " 'Elizabeth Bennet': 'PERSON',\n",
       " 'Come': 'ORGANIZATION',\n",
       " 'Darcy': 'PERSON',\n",
       " 'Catherine': 'ORGANIZATION',\n",
       " 'Longbourn': 'ORGANIZATION',\n",
       " 'Everybody': 'PERSON',\n",
       " 'Miss Lucas': 'PERSON',\n",
       " 'Miss King': 'PERSON',\n",
       " 'Maria Lucas': 'PERSON',\n",
       " 'God': 'PERSON',\n",
       " 'Netherfield House': 'FACILITY',\n",
       " 'Bingley': 'PERSON',\n",
       " 'Meryton': 'GPE',\n",
       " 'Miss Bennet': 'PERSON',\n",
       " 'Bennets': 'ORGANIZATION',\n",
       " 'Sir William Lucas': 'PERSON',\n",
       " 'Lucas Lodge': 'PERSON',\n",
       " 'St. James': 'ORGANIZATION',\n",
       " 'Miss Lucases': 'ORGANIZATION',\n",
       " 'Miss Bennets': 'PERSON',\n",
       " '_You_': 'PERSON',\n",
       " 'Charlotte': 'PERSON',\n",
       " 'Mr.': 'PERSON',\n",
       " 'Mr. Robinson': 'PERSON',\n",
       " 'Eliza': 'PERSON',\n",
       " 'Vanity': 'PERSON',\n",
       " 'Lucas': 'PERSON',\n",
       " 'Commerce': 'ORGANIZATION',\n",
       " 'Sir': 'GPE',\n",
       " 'William Lucas': 'PERSON',\n",
       " 'Colonel Forster': 'ORGANIZATION',\n",
       " 'Colonel': 'ORGANIZATION',\n",
       " 'Scotch': 'GPE',\n",
       " 'Irish': 'GPE',\n",
       " 'Lucases': 'ORGANIZATION',\n",
       " 'William': 'PERSON',\n",
       " 'Lady': 'PERSON',\n",
       " 'Miss Eliza': 'PERSON',\n",
       " 'Miss Elizabeth': 'ORGANIZATION',\n",
       " 'Pemberley': 'ORGANIZATION',\n",
       " 'Mr. Phillips': 'PERSON',\n",
       " 'Captain Carter': 'PERSON',\n",
       " 'Miss Watson': 'PERSON',\n",
       " 'Clarke': 'GPE',\n",
       " 'MY': 'ORGANIZATION',\n",
       " 'Louisa': 'PERSON',\n",
       " 'BINGLEY': 'ORGANIZATION',\n",
       " 'Breakfast': 'PERSON',\n",
       " 'Mr. Jones': 'PERSON',\n",
       " 'Captain': 'GPE',\n",
       " 'Miss Elizabeth Bennet': 'PERSON',\n",
       " 'Miss Jane Bennet': 'PERSON',\n",
       " 'Eliza Bennet': 'PERSON',\n",
       " 'Charles': 'PERSON',\n",
       " 'Caroline': 'PERSON',\n",
       " 'Miss Darcy': 'PERSON',\n",
       " 'Madam': 'PERSON',\n",
       " 'Mamma': 'PERSON',\n",
       " 'Charlotte Lucas': 'PERSON',\n",
       " '_That_': 'PERSON',\n",
       " 'Gardiner': 'PERSON',\n",
       " 'Carter': 'PERSON',\n",
       " 'Hurst': 'PERSON',\n",
       " 'How': 'PERSON',\n",
       " 'Tell': 'ORGANIZATION',\n",
       " 'Miss': 'PERSON',\n",
       " 'Nay': 'PERSON',\n",
       " 'Italian': 'GPE',\n",
       " 'Phillips': 'PERSON',\n",
       " 'Nicholls': 'PERSON',\n",
       " 'Miss Eliza Bennet': 'ORGANIZATION',\n",
       " 'Intimate': 'ORGANIZATION',\n",
       " 'Tease': 'PERSON',\n",
       " 'Implacable': 'ORGANIZATION',\n",
       " 'Steady': 'PERSON',\n",
       " 'Much': 'PERSON',\n",
       " 'Mr. Collins': 'PERSON',\n",
       " 'Westerham': 'GPE',\n",
       " 'Kent': 'GPE',\n",
       " 'Easter': 'PERSON',\n",
       " 'Right Honourable Lady Catherine': 'ORGANIZATION',\n",
       " 'Bourgh': 'GPE',\n",
       " 'Sir Lewis de Bourgh': 'PERSON',\n",
       " 'Church': 'ORGANIZATION',\n",
       " 'Lady Catherine': 'PERSON',\n",
       " 'COLLINS': 'ORGANIZATION',\n",
       " 'No': 'ORGANIZATION',\n",
       " 'Rosings': 'GPE',\n",
       " 'Rosings Park': 'GPE',\n",
       " 'Her': 'PERSON',\n",
       " 'British': 'GPE',\n",
       " 'Fordyce': 'PERSON',\n",
       " 'Richard': 'PERSON',\n",
       " 'Mr. Denny': 'PERSON',\n",
       " 'Hunsford': 'GPE',\n",
       " 'Mr. Wickham': 'PERSON',\n",
       " 'Mr. Phillip': 'PERSON',\n",
       " 'Wickham': 'PERSON',\n",
       " 'Denny': 'PERSON',\n",
       " 'Society': 'PERSON',\n",
       " 'Family': 'PERSON',\n",
       " 'Pemberley House': 'PERSON',\n",
       " 'Lady Anne Darcy': 'PERSON',\n",
       " 'Vain': 'PERSON',\n",
       " 'Collins': 'PERSON',\n",
       " 'Longbourn House': 'FACILITY',\n",
       " 'Archbishop': 'ORGANIZATION',\n",
       " 'Hunsford Parsonage': 'ORGANIZATION',\n",
       " 'Bingleys': 'ORGANIZATION',\n",
       " 'first.': 'ORGANIZATION',\n",
       " 'George Wickham': 'PERSON',\n",
       " 'Meryton.': 'ORGANIZATION',\n",
       " 'Miss de Bourgh': 'PERSON',\n",
       " 'Jenkinson': 'PERSON',\n",
       " 'Choose': 'PERSON',\n",
       " 'Aye': 'PERSON',\n",
       " 'York': 'ORGANIZATION',\n",
       " 'Miss Lizzy': 'PERSON',\n",
       " 'Caroline Bingley': 'PERSON',\n",
       " 'Grosvenor Street': 'GPE',\n",
       " 'Georgiana Darcy': 'PERSON',\n",
       " 'Such': 'PERSON',\n",
       " 'Good Lord': 'PERSON',\n",
       " 'Between Elizabeth': 'PERSON',\n",
       " 'Hope': 'PERSON',\n",
       " 'Thank God': 'PERSON',\n",
       " 'Christmas': 'ORGANIZATION',\n",
       " 'Mr. Gardiner': 'PERSON',\n",
       " 'Pray': 'PERSON',\n",
       " 'Poor Jane': 'PERSON',\n",
       " 'Gracechurch Street': 'FACILITY',\n",
       " 'Gardiners': 'ORGANIZATION',\n",
       " 'Phillipses': 'ORGANIZATION',\n",
       " 'Mr. Fitzwilliam Darcy': 'PERSON',\n",
       " 'Maria': 'GPE',\n",
       " 'March': 'PERSON',\n",
       " 'Grosvenor': 'GPE',\n",
       " 'Absence': 'PERSON',\n",
       " 'Gracechurch': 'GPE',\n",
       " 'Thank Heaven': 'PERSON',\n",
       " 'Adieu': 'PERSON',\n",
       " 'Lakes': 'GPE',\n",
       " 'Parsonage': 'ORGANIZATION',\n",
       " 'Make': 'PERSON',\n",
       " 'Sir Lewis': 'PERSON',\n",
       " 'Lewis de Bourgh': 'PERSON',\n",
       " 'Miss Pope': 'PERSON',\n",
       " 'Hunsford Lane': 'ORGANIZATION',\n",
       " 'Park': 'ORGANIZATION',\n",
       " 'Colonel Fitzwilliam': 'ORGANIZATION',\n",
       " 'Fitzwilliam': 'PERSON',\n",
       " 'Georgiana': 'PERSON',\n",
       " 'Anne': 'PERSON',\n",
       " 'Mrs. Collins': 'PERSON',\n",
       " 'Longbourn.': 'ORGANIZATION',\n",
       " 'Did Mr. Darcy': 'PERSON',\n",
       " 'Wilfully': 'PERSON',\n",
       " 'Cambridge': 'ORGANIZATION',\n",
       " 'Ramsgate': 'ORGANIZATION',\n",
       " 'Regard': 'PERSON',\n",
       " 'FITZWILLIAM': 'ORGANIZATION',\n",
       " 'Militia': 'PERSON',\n",
       " 'Dawson': 'PERSON',\n",
       " 'Lady Anne': 'PERSON',\n",
       " 'John': 'PERSON',\n",
       " 'Bromley': 'PERSON',\n",
       " 'Bell': 'ORGANIZATION',\n",
       " 'Anxiety': 'PERSON',\n",
       " 'Rosings.': 'ORGANIZATION',\n",
       " 'Poor Charlotte': 'PERSON',\n",
       " 'Good': 'ORGANIZATION',\n",
       " 'Brighton': 'GPE',\n",
       " 'Mary King': 'PERSON',\n",
       " 'Liverpool': 'ORGANIZATION',\n",
       " 'Lord': 'PERSON',\n",
       " 'Harriet': 'PERSON',\n",
       " 'Pen': 'PERSON',\n",
       " 'Pratt': 'GPE',\n",
       " 'George': 'GPE',\n",
       " 'Dear Lizzy': 'PERSON',\n",
       " 'Collinses': 'ORGANIZATION',\n",
       " '_They_': 'PERSON',\n",
       " 'Good Heaven': 'PERSON',\n",
       " 'Colonel Miller': 'PERSON',\n",
       " 'Had Lydia': 'PERSON',\n",
       " 'Respect': 'PERSON',\n",
       " 'War Office': 'FACILITY',\n",
       " 'July': 'GPE',\n",
       " 'Matlock': 'GPE',\n",
       " 'Chatsworth': 'PERSON',\n",
       " 'Dovedale': 'GPE',\n",
       " 'Peak': 'ORGANIZATION',\n",
       " 'Oxford': 'GPE',\n",
       " 'Blenheim': 'GPE',\n",
       " 'Warwick': 'PERSON',\n",
       " 'Kenilworth': 'PERSON',\n",
       " 'Birmingham': 'GPE',\n",
       " 'Lambton': 'PERSON',\n",
       " 'Pemberley Woods': 'PERSON',\n",
       " 'Reynolds': 'PERSON',\n",
       " 'Bakewell': 'ORGANIZATION',\n",
       " 'Dove Dale': 'PERSON',\n",
       " 'Netherfield.': 'ORGANIZATION',\n",
       " 'Spanish': 'GPE',\n",
       " 'Annesley': 'PERSON',\n",
       " 'Scotland': 'GPE',\n",
       " 'Dearest Lizzy': 'PERSON',\n",
       " 'Gretna Green': 'GPE',\n",
       " 'Clapham': 'PERSON',\n",
       " 'Epsom': 'PERSON',\n",
       " 'Barnet': 'GPE',\n",
       " 'Hatfield': 'PERSON',\n",
       " 'Colonel F.': 'PERSON',\n",
       " 'Poor Kitty': 'PERSON',\n",
       " 'Surprise': 'PERSON',\n",
       " 'Had Elizabeth': 'PERSON',\n",
       " 'Forsters': 'ORGANIZATION',\n",
       " 'Poor': 'PERSON',\n",
       " 'Could Colonel Forster': 'PERSON',\n",
       " 'LYDIA': 'ORGANIZATION',\n",
       " 'Rendered': 'PERSON',\n",
       " 'Eastbourne': 'ORGANIZATION',\n",
       " 'Balls': 'PERSON',\n",
       " 'Hill': 'PERSON',\n",
       " 'Haggerston': 'PERSON',\n",
       " 'Stay': 'PERSON',\n",
       " 'Girls': 'PERSON',\n",
       " 'Miss Lydia': 'PERSON',\n",
       " 'Poor Lydia': 'PERSON',\n",
       " 'Great': 'GPE',\n",
       " 'Miss Lydia Bennet': 'PERSON',\n",
       " 'Haye Park': 'PERSON',\n",
       " 'Gouldings': 'ORGANIZATION',\n",
       " 'Stoke': 'FACILITY',\n",
       " 'Ashworth': 'PERSON',\n",
       " 'Pulvis Lodge': 'PERSON',\n",
       " 'Into': 'ORGANIZATION',\n",
       " \"Mr. Bennet's\": 'PERSON',\n",
       " 'General': 'GPE',\n",
       " 'North': 'LOCATION',\n",
       " 'South': 'LOCATION',\n",
       " 'Smiles': 'PERSON',\n",
       " 'William Goulding': 'PERSON',\n",
       " 'Newcastle': 'GPE',\n",
       " 'Are': 'PERSON',\n",
       " 'St. Clement': 'ORGANIZATION',\n",
       " 'Little': 'ORGANIZATION',\n",
       " 'Mr. Stone': 'PERSON',\n",
       " '_I_': 'PERSON',\n",
       " '_He_': 'PERSON',\n",
       " 'Poor Reynolds': 'ORGANIZATION',\n",
       " 'Kympton': 'GPE',\n",
       " 'Parsonage House': 'ORGANIZATION',\n",
       " 'Happy': 'PERSON',\n",
       " 'Times': 'ORGANIZATION',\n",
       " 'Esq': 'PERSON',\n",
       " 'Anxious': 'PERSON',\n",
       " 'Scarborough': 'GPE',\n",
       " 'Well': 'ORGANIZATION',\n",
       " 'French': 'GPE',\n",
       " 'Sarah': 'PERSON',\n",
       " 'Where': 'PERSON',\n",
       " 'Darcy_': 'PERSON',\n",
       " 'Never': 'PERSON',\n",
       " 'Christian': 'GPE',\n",
       " 'Pemberley.': 'ORGANIZATION',\n",
       " 'Oakham Mount': 'PERSON',\n",
       " 'Mount': 'GPE',\n",
       " 'Yours': 'PERSON',\n",
       " 'DEAR': 'ORGANIZATION',\n",
       " 'Console Lady Catherine': 'PERSON',\n",
       " 'Bath': 'GPE'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Disambiguation of entity with entity types\n",
    "ne_chunked = nltk.ne_chunk(tagged, binary=False)\n",
    "extractEntities(ne_chunked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.util import *\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "bneu = \"\"\n",
    "neu = 0\n",
    "bneg = \"\"\n",
    "neg = 0\n",
    "bpos = \"\"\n",
    "pos = 0\n",
    "\n",
    "\n",
    "vader_analyzer = SentimentIntensityAnalyzer()\n",
    "for sentense in nltk.sent_tokenize(text):\n",
    "    d = vader_analyzer.polarity_scores(text)\n",
    "    if d['neg'] > neg:\n",
    "        neg = d['neg']\n",
    "        bneg = sentense\n",
    "    if d['pos'] > pos:\n",
    "        pos = d['pos']\n",
    "        bpos = sentense\n",
    "    if d['neu'] > neu:\n",
    "        neu = d['neu']\n",
    "        bneu = sentense\n",
    "\n",
    "print(\"neg:\", neg, bneg)\n",
    "print(\"neu:\", neu, bneu)\n",
    "print(\"pos:\", pos, bpos)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings - Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /home/users/m/martilad/mi-ddw/t2/__venv__/lib/python3.7/site-packages (3.7.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/users/m/martilad/mi-ddw/t2/__venv__/lib/python3.7/site-packages (from gensim) (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/users/m/martilad/mi-ddw/t2/__venv__/lib/python3.7/site-packages (from gensim) (1.16.2)\n",
      "Requirement already satisfied: six>=1.5.0 in /home/users/m/martilad/mi-ddw/t2/__venv__/lib/python3.7/site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: smart-open>=1.7.0 in /home/users/m/martilad/mi-ddw/t2/__venv__/lib/python3.7/site-packages (from gensim) (1.8.0)\n",
      "Requirement already satisfied: boto>=2.32 in /home/users/m/martilad/mi-ddw/t2/__venv__/lib/python3.7/site-packages (from smart-open>=1.7.0->gensim) (2.49.0)\n",
      "Requirement already satisfied: bz2file in /home/users/m/martilad/mi-ddw/t2/__venv__/lib/python3.7/site-packages (from smart-open>=1.7.0->gensim) (0.98)\n",
      "Requirement already satisfied: requests in /home/users/m/martilad/mi-ddw/t2/__venv__/lib/python3.7/site-packages (from smart-open>=1.7.0->gensim) (2.21.0)\n",
      "Requirement already satisfied: boto3 in /home/users/m/martilad/mi-ddw/t2/__venv__/lib/python3.7/site-packages (from smart-open>=1.7.0->gensim) (1.9.111)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/users/m/martilad/mi-ddw/t2/__venv__/lib/python3.7/site-packages (from requests->smart-open>=1.7.0->gensim) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/users/m/martilad/mi-ddw/t2/__venv__/lib/python3.7/site-packages (from requests->smart-open>=1.7.0->gensim) (1.24.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/users/m/martilad/mi-ddw/t2/__venv__/lib/python3.7/site-packages (from requests->smart-open>=1.7.0->gensim) (2019.3.9)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/users/m/martilad/mi-ddw/t2/__venv__/lib/python3.7/site-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /home/users/m/martilad/mi-ddw/t2/__venv__/lib/python3.7/site-packages (from boto3->smart-open>=1.7.0->gensim) (0.2.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/users/m/martilad/mi-ddw/t2/__venv__/lib/python3.7/site-packages (from boto3->smart-open>=1.7.0->gensim) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.111 in /home/users/m/martilad/mi-ddw/t2/__venv__/lib/python3.7/site-packages (from boto3->smart-open>=1.7.0->gensim) (1.12.111)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /home/users/m/martilad/mi-ddw/t2/__venv__/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.111->boto3->smart-open>=1.7.0->gensim) (2.8.0)\n",
      "Requirement already satisfied: docutils>=0.10 in /home/users/m/martilad/mi-ddw/t2/__venv__/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.111->boto3->smart-open>=1.7.0->gensim) (0.14)\n"
     ]
    }
   ],
   "source": [
    "! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "tokens = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "# data\n",
    "from nltk.corpus import brown\n",
    "data = brown.sents()\n",
    "\n",
    "# Create CBOW model \n",
    "cbow_model = gensim.models.Word2Vec(tokens, min_count = 5, size = 100, window = 5, workers=10) \n",
    "  \n",
    "# Create Skip Gram model \n",
    "sg_model = gensim.models.Word2Vec(tokens, min_count = 5, size = 100, window = 5, sg = 1, workers=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('often', 0.9995832443237305),\n",
       " ('us', 0.9995463490486145),\n",
       " ('consider', 0.9995065331459045),\n",
       " ('married', 0.9993481040000916),\n",
       " ('allow', 0.9992833137512207)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.wv.most_similar(\"love\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sick', 0.9970794916152954),\n",
       " ('grave', 0.9968978762626648),\n",
       " ('sing', 0.9963023662567139),\n",
       " ('separate', 0.9946496486663818),\n",
       " ('uncivil', 0.9943731427192688)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_model.wv.most_similar(\"dead\", topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in /home/users/m/martilad/mi-ddw/t2/__venv__/lib/python3.7/site-packages (1.5.0)\n",
      "Collecting Image\n",
      "  Using cached https://files.pythonhosted.org/packages/0c/ec/51969468a8b87f631cc0e60a6bf1e5f6eec8ef3fd2ee45dc760d5a93b82a/image-1.5.27-py2.py3-none-any.whl\n",
      "Collecting matplotlib\n",
      "  Using cached https://files.pythonhosted.org/packages/83/2a/e47bbd9396af32376863a426baed62d9bf3091f81defd1fe81c5f33b11a3/matplotlib-3.0.3-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: pillow in /home/users/m/martilad/mi-ddw/t2/__venv__/lib/python3.7/site-packages (from wordcloud) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /home/users/m/martilad/mi-ddw/t2/__venv__/lib/python3.7/site-packages (from wordcloud) (1.16.2)\n",
      "Requirement already satisfied: django in /home/users/m/martilad/mi-ddw/t2/__venv__/lib/python3.7/site-packages (from Image) (2.1.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/users/m/martilad/mi-ddw/t2/__venv__/lib/python3.7/site-packages (from matplotlib) (2.8.0)\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib)\n",
      "  Using cached https://files.pythonhosted.org/packages/5c/7e/d6cae2f241ba474a2665f24b480bf4e247036d63939dda2bbc4d2ee5069d/kiwisolver-1.0.1-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib)\n",
      "  Using cached https://files.pythonhosted.org/packages/de/0a/001be530836743d8be6c2d85069f46fecf84ac6c18c7f5fb8125ee11d854/pyparsing-2.3.1-py2.py3-none-any.whl\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pytz in /home/users/m/martilad/mi-ddw/t2/__venv__/lib/python3.7/site-packages (from django->Image) (2018.9)\n",
      "Requirement already satisfied: six>=1.5 in /home/users/m/martilad/mi-ddw/t2/__venv__/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /home/users/m/martilad/mi-ddw/t2/__venv__/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (39.0.1)\n",
      "Installing collected packages: Image, kiwisolver, pyparsing, cycler, matplotlib\n",
      "Successfully installed Image-1.5.27 cycler-0.10.0 kiwisolver-1.0.1 matplotlib-3.0.3 pyparsing-2.3.1\n"
     ]
    }
   ],
   "source": [
    "! pip install wordcloud Image matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from os import path\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wordcloud = WordCloud().generate(text)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text.txt', 'r') as myfile:\n",
    "    book = myfile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 9132),\n",
       " ('to', 4086),\n",
       " ('.', 4085),\n",
       " ('the', 4058),\n",
       " ('of', 3596),\n",
       " ('and', 3423),\n",
       " ('her', 2107),\n",
       " ('I', 2065),\n",
       " ('a', 1897),\n",
       " ('was', 1837)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "tokens = nltk.word_tokenize(book)\n",
    "def tokenCounts(tokens):\n",
    "    counts = Counter(tokens)\n",
    "    sortedCounts = sorted(counts.items(), key=lambda count:count[1], reverse=True)\n",
    "    return sortedCounts\n",
    "\n",
    "tokenCounts(tokens)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 2065),\n",
       " ('Mr.', 768),\n",
       " ('Elizabeth', 631),\n",
       " (\"'s\", 574),\n",
       " ('could', 513),\n",
       " ('would', 465),\n",
       " ('said', 401),\n",
       " ('Darcy', 401),\n",
       " ('Mrs.', 337),\n",
       " ('She', 324)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "punctuation += '“”--'\n",
    "stops = stopwords.words('english')\n",
    "tokens = nltk.word_tokenize(book)\n",
    "\n",
    "filtered_tokens = [token for token in tokens if token not in punctuation]\n",
    "filtered_tokens = [token for token in filtered_tokens if token not in stops]\n",
    "tokenCounts(filtered_tokens)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['“', 'Oh', '!'], ['She', 'is', 'the', 'most', 'beautiful', 'creature', 'I', 'ever', 'beheld', '!'], ['But', 'there', 'is', 'one', 'of', 'her', 'sisters', 'sitting', 'down', 'just', 'behind', 'you', ',', 'who', 'is', 'very', 'pretty', ',', 'and', 'I', 'dare', 'say', 'very', 'agreeable', '.'], ['Do', 'let', 'me', 'ask', 'my', 'partner', 'to', 'introduce', 'you.', '”', '“', 'Which', 'do', 'you', 'mean', '?', '”', 'and', 'turning', 'round', 'he', 'looked', 'for', 'a', 'moment', 'at', 'Elizabeth', ',', 'till', 'catching', 'her', 'eye', ',', 'he', 'withdrew', 'his', 'own', 'and', 'coldly', 'said', ':', '“', 'She', 'is', 'tolerable', ',', 'but', 'not', 'handsome', 'enough', 'to', 'tempt', '_me_', ';', 'I', 'am', 'in', 'no', 'humour', 'at', 'present', 'to', 'give', 'consequence', 'to', 'young', 'ladies', 'who', 'are', 'slighted', 'by', 'other', 'men', '.'], ['You', 'had', 'better', 'return', 'to', 'your', 'partner', 'and', 'enjoy', 'her', 'smiles', ',', 'for', 'you', 'are', 'wasting', 'your', 'time', 'with', 'me.', '”', 'Mr.', 'Bingley', 'followed', 'his', 'advice', '.']]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-0dd1fb4a6056>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m105\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtokenCounts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-38-b46ce6214feb>\u001b[0m in \u001b[0;36mtokenCounts\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenCounts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0msortedCounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msortedCounts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.7/collections/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'expected at most 1 arguments, got %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__missing__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.7/collections/__init__.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    651\u001b[0m                     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fast path when counter is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m                 \u001b[0m_count_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(book)\n",
    "tokens = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "print(tokens[100:105])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('“', 'JJ'), ('Oh', 'UH'), ('!', '.')], [('She', 'PRP'), ('is', 'VBZ'), ('the', 'DT'), ('most', 'RBS'), ('beautiful', 'JJ'), ('creature', 'NN'), ('I', 'PRP'), ('ever', 'RB'), ('beheld', 'VBD'), ('!', '.')], [('But', 'CC'), ('there', 'EX'), ('is', 'VBZ'), ('one', 'CD'), ('of', 'IN'), ('her', 'PRP$'), ('sisters', 'NNS'), ('sitting', 'VBG'), ('down', 'RP'), ('just', 'RB'), ('behind', 'IN'), ('you', 'PRP'), (',', ','), ('who', 'WP'), ('is', 'VBZ'), ('very', 'RB'), ('pretty', 'RB'), (',', ','), ('and', 'CC'), ('I', 'PRP'), ('dare', 'VBP'), ('say', 'VB'), ('very', 'RB'), ('agreeable', 'JJ'), ('.', '.')], [('Do', 'VB'), ('let', 'VB'), ('me', 'PRP'), ('ask', 'VB'), ('my', 'PRP$'), ('partner', 'NN'), ('to', 'TO'), ('introduce', 'VB'), ('you.', 'JJ'), ('”', 'NNP'), ('“', 'NNP'), ('Which', 'NNP'), ('do', 'VBP'), ('you', 'PRP'), ('mean', 'VB'), ('?', '.'), ('”', 'NN'), ('and', 'CC'), ('turning', 'VBG'), ('round', 'NN'), ('he', 'PRP'), ('looked', 'VBD'), ('for', 'IN'), ('a', 'DT'), ('moment', 'NN'), ('at', 'IN'), ('Elizabeth', 'NNP'), (',', ','), ('till', 'NN'), ('catching', 'VBG'), ('her', 'PRP$'), ('eye', 'NN'), (',', ','), ('he', 'PRP'), ('withdrew', 'VBD'), ('his', 'PRP$'), ('own', 'JJ'), ('and', 'CC'), ('coldly', 'RB'), ('said', 'VBD'), (':', ':'), ('“', 'NN'), ('She', 'PRP'), ('is', 'VBZ'), ('tolerable', 'JJ'), (',', ','), ('but', 'CC'), ('not', 'RB'), ('handsome', 'JJ'), ('enough', 'RB'), ('to', 'TO'), ('tempt', 'VB'), ('_me_', 'NNP'), (';', ':'), ('I', 'PRP'), ('am', 'VBP'), ('in', 'IN'), ('no', 'DT'), ('humour', 'NN'), ('at', 'IN'), ('present', 'JJ'), ('to', 'TO'), ('give', 'VB'), ('consequence', 'NN'), ('to', 'TO'), ('young', 'JJ'), ('ladies', 'NNS'), ('who', 'WP'), ('are', 'VBP'), ('slighted', 'VBN'), ('by', 'IN'), ('other', 'JJ'), ('men', 'NNS'), ('.', '.')], [('You', 'PRP'), ('had', 'VBD'), ('better', 'JJR'), ('return', 'NN'), ('to', 'TO'), ('your', 'PRP$'), ('partner', 'NN'), ('and', 'CC'), ('enjoy', 'VB'), ('her', 'PRP$'), ('smiles', 'NNS'), (',', ','), ('for', 'IN'), ('you', 'PRP'), ('are', 'VBP'), ('wasting', 'VBG'), ('your', 'PRP$'), ('time', 'NN'), ('with', 'IN'), ('me.', 'NN'), ('”', 'VBP'), ('Mr.', 'NNP'), ('Bingley', 'NNP'), ('followed', 'VBD'), ('his', 'PRP$'), ('advice', 'NN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(book)\n",
    "tokens = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "tagged = [nltk.pos_tag(sent) for sent in tokens]\n",
    "\n",
    "print(tagged[100:105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset('JJ*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "stems = {token:stemmer.stem(token) for token in tokens}\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "lemmas = {token:lemmatizer.lemmatize(token) for token in tokens}\n",
    "print(lemmas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
