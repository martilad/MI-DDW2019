{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/martilad/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/martilad/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/martilad/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/martilad/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# download packages for nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.drawing.nx_agraph import graphviz_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the text\n",
    "texts = []\n",
    "for i in range(1, 16, 1):\n",
    "    text = None\n",
    "    with open('../data/t'+ str(i) + '.txt', 'r') as f:\n",
    "        text = f.read()\n",
    "    texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_graph(G, node1, node2):\n",
    "    \"\"\"Function for add edge and nodes to graph.\n",
    "    - sort the name of node - it is non oriented graph\n",
    "    - id node not in in graph add\n",
    "    - each edge add to same nodes increase weight on the edge    \n",
    "    \"\"\"\n",
    "    if node1 > node2:\n",
    "        tmp = node1\n",
    "        node1 = node2\n",
    "        node2 = tmp\n",
    "    if node1 not in G:\n",
    "        G.add_node(node1)\n",
    "        \n",
    "    if node2 not in G:\n",
    "        G.add_node(node2)\n",
    "        \n",
    "    if node2 not in G[node1]:\n",
    "        G.add_edge(node1, node2)\n",
    "        G[node1][node2]['weight'] = 0\n",
    "    \n",
    "    G[node1][node2]['weight'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in dataset:  15267\n"
     ]
    }
   ],
   "source": [
    "# Do POS tagging on the whole text \n",
    "all_text = ''\n",
    "for i in texts:\n",
    "    all_text += i\n",
    "tokens = nltk.word_tokenize(all_text)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "print(\"Number of words in dataset: \", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in dataset:  619\n"
     ]
    }
   ],
   "source": [
    "# Load text to sentences and do POS tag on it\n",
    "sentences_in_text = []\n",
    "sent_tokens_in_text = []\n",
    "sent_tagged_in_text = []\n",
    "cnt = 0\n",
    "for i in texts:\n",
    "    sentences = nltk.sent_tokenize(i)\n",
    "    cnt += len(sentences)\n",
    "    sentences_in_text.append(sentences)\n",
    "    sent_tokens = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sent_tokens_in_text.append(sent_tokens)\n",
    "    sent_tagged = [nltk.pos_tag(sent) for sent in sent_tokens]\n",
    "    sent_tagged_in_text.append(sent_tagged)\n",
    "print(\"Number of sentences in dataset: \", cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting words per category in dict\n",
    "def count_words_per_category(counter_dict, category, word):\n",
    "    if category not in counter_dict:\n",
    "        counter_dict[category] = {}\n",
    "    if word not in counter_dict[category]:\n",
    "        counter_dict[category][word] = 0\n",
    "    counter_dict[category][word] += 1\n",
    "    \n",
    "# Print n best for each category from counter dict|\n",
    "def print_best_n_for_each_category(cnt, n):\n",
    "    for i in cnt:\n",
    "        sorted_x = sorted(cnt[i].items(), key=lambda kv: kv[1])\n",
    "        print(i,\": \", sep='')\n",
    "        for j in range(n if len(sorted_x)>=n else len(sorted_x)):\n",
    "            print(' ', sorted_x[-(j+1)][0],'-',sorted_x[-(j+1)][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract entities from ne_chunked format to dict\n",
    "def extractEntities(ne_chunked):\n",
    "    data = {}\n",
    "    for entity in ne_chunked:\n",
    "        if isinstance(entity, nltk.tree.Tree):\n",
    "            text = \" \".join([word for word, tag in entity.leaves()])\n",
    "            ent = entity.label()\n",
    "            data[text] = ent\n",
    "        else:\n",
    "            continue\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best word in POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: \n",
      "  hydrogen - 32\n",
      "  deal - 29\n",
      "  time - 23\n",
      "  electricity - 21\n",
      "  manager - 20\n",
      ":: \n",
      "  - - 86\n",
      "  : - 41\n",
      "  ; - 2\n",
      "  ... - 1\n",
      "IN: \n",
      "  of - 334\n",
      "  in - 245\n",
      "  on - 110\n",
      "  for - 104\n",
      "  at - 101\n",
      "NNP: \n",
      "  United - 83\n",
      "  Solskjaer - 68\n",
      "  ’ - 39\n",
      "  Mourinho - 28\n",
      "  League - 25\n",
      "VBP: \n",
      "  have - 54\n",
      "  are - 45\n",
      "  do - 10\n",
      "  ’ - 9\n",
      "  say - 7\n",
      "DT: \n",
      "  the - 704\n",
      "  a - 307\n",
      "  The - 96\n",
      "  an - 46\n",
      "  this - 39\n",
      "VBZ: \n",
      "  is - 110\n",
      "  has - 84\n",
      "  says - 24\n",
      "  ’ - 9\n",
      "  does - 9\n",
      "TO: \n",
      "  to - 332\n",
      "VB: \n",
      "  be - 81\n",
      "  have - 18\n",
      "  make - 10\n",
      "  get - 10\n",
      "  happen - 8\n",
      "JJ: \n",
      "  first - 36\n",
      "  last - 17\n",
      "  new - 15\n",
      "  final - 14\n",
      "  clean - 13\n",
      "NNS: \n",
      "  people - 29\n",
      "  islands - 22\n",
      "  years - 18\n",
      "  villages - 12\n",
      "  results - 12\n",
      ".: \n",
      "  . - 578\n",
      "  ? - 41\n",
      "  ! - 2\n",
      "NNPS: \n",
      "  Orcadians - 4\n",
      "  Commons - 3\n",
      "  Rights - 1\n",
      "  States - 1\n",
      "  Indians - 1\n",
      "VBD: \n",
      "  was - 101\n",
      "  had - 64\n",
      "  said - 59\n",
      "  were - 33\n",
      "  did - 18\n",
      "RB: \n",
      "  not - 53\n",
      "  n't - 31\n",
      "  also - 25\n",
      "  only - 23\n",
      "  just - 19\n",
      "VBG: \n",
      "  including - 15\n",
      "  according - 11\n",
      "  winning - 7\n",
      "  being - 7\n",
      "  looking - 5\n",
      "CD: \n",
      "  one - 26\n",
      "  two - 15\n",
      "  three - 13\n",
      "  five - 11\n",
      "  eight - 9\n",
      "CC: \n",
      "  and - 252\n",
      "  But - 41\n",
      "  but - 40\n",
      "  or - 25\n",
      "  And - 15\n",
      "VBN: \n",
      "  been - 53\n",
      "  made - 12\n",
      "  seen - 11\n",
      "  gone - 7\n",
      "  done - 6\n",
      "POS: \n",
      "  's - 144\n",
      "  ' - 7\n",
      "  'ate - 1\n",
      "  'Solskjaer - 1\n",
      ",: \n",
      "  , - 626\n",
      "PRP: \n",
      "  he - 85\n",
      "  it - 82\n",
      "  they - 48\n",
      "  I - 35\n",
      "  It - 34\n",
      "PRP$: \n",
      "  their - 49\n",
      "  his - 47\n",
      "  its - 40\n",
      "  her - 16\n",
      "  my - 11\n",
      "EX: \n",
      "  There - 15\n",
      "  there - 13\n",
      "MD: \n",
      "  will - 48\n",
      "  would - 46\n",
      "  could - 32\n",
      "  can - 26\n",
      "  should - 15\n",
      "``: \n",
      "  `` - 88\n",
      "'': \n",
      "  '' - 86\n",
      "  ' - 1\n",
      "WRB: \n",
      "  when - 29\n",
      "  where - 16\n",
      "  how - 15\n",
      "  When - 8\n",
      "  why - 4\n",
      "WP: \n",
      "  who - 45\n",
      "  what - 27\n",
      "  What - 12\n",
      "WDT: \n",
      "  which - 38\n",
      "  that - 26\n",
      "  whatever - 1\n",
      "WP$: \n",
      "  whose - 3\n",
      "RBR: \n",
      "  more - 5\n",
      "  later - 2\n",
      "  More - 1\n",
      "  less - 1\n",
      "  better - 1\n",
      "JJR: \n",
      "  more - 18\n",
      "  cheaper - 3\n",
      "  less - 3\n",
      "  More - 3\n",
      "  harder - 3\n",
      "RP: \n",
      "  up - 14\n",
      "  out - 10\n",
      "  off - 8\n",
      "  down - 5\n",
      "  over - 3\n",
      "JJS: \n",
      "  most - 9\n",
      "  least - 6\n",
      "  largest - 4\n",
      "  best - 2\n",
      "  highest - 2\n",
      "RBS: \n",
      "  most - 8\n",
      "(: \n",
      "  ( - 22\n",
      "): \n",
      "  ) - 22\n",
      "PDT: \n",
      "  all - 2\n",
      "  both - 2\n",
      "  s - 1\n",
      "  half - 1\n",
      "  such - 1\n",
      "$: \n",
      "  $ - 4\n",
      "UH: \n",
      "  Yes - 1\n",
      "FW: \n",
      "  s - 1\n"
     ]
    }
   ],
   "source": [
    "# Count the words in each tags\n",
    "cnt = {}\n",
    "for text in sent_tagged_in_text:\n",
    "    for sent in text:\n",
    "        for word in sent:\n",
    "            count_words_per_category(cnt, word[1], word[0])\n",
    "print_best_n_for_each_category(cnt, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best word in NER using ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the entities for each sentence and for each article\n",
    "NER_nltk = []\n",
    "for sent_tag in sent_tagged_in_text:\n",
    "    # Find the entities for each sentence\n",
    "    NER_nltk_sent = []\n",
    "    ne_chunked_sent = nltk.ne_chunk_sents(sent_tagged)\n",
    "    for i in ne_chunked_sent:\n",
    "        NER_nltk_sent.append(extractEntities(i))\n",
    "    NER_nltk.append(NER_nltk_sent)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPE: \n",
      "  Shapinsay - 60\n",
      "  Orkney - 60\n",
      "  Scottish - 30\n",
      "  Stromness - 30\n",
      "  Scotland - 30\n",
      "PERSON: \n",
      "  Orkney - 150\n",
      "  Bews - 45\n",
      "  Lidderdale - 30\n",
      "  Stockan - 15\n",
      "  Clipsham - 15\n",
      "ORGANIZATION: \n",
      "  EMEC - 75\n",
      "  UK - 60\n",
      "  IMO - 30\n",
      "  CCS - 30\n",
      "  Orcadians - 30\n",
      "LOCATION: \n",
      "  North Sea - 15\n",
      "  Scotland - 15\n"
     ]
    }
   ],
   "source": [
    "cnt_NER = {}\n",
    "for text in NER_nltk:\n",
    "    for sent in text:\n",
    "        for i in sent:\n",
    "            count_words_per_category(cnt_NER, sent[i], i)\n",
    "print_best_n_for_each_category(cnt_NER, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My own paterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_entity_sent(tagged, pattern):\n",
    "    sent_entity = []\n",
    "    cp = nltk.RegexpParser(pattern)\n",
    "    for j in tagged:\n",
    "        sent_entity.append(extractEntities(cp.parse(j)))\n",
    "    return sent_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the entities for each sentence and for each article\n",
    "NER_nltk2 = []\n",
    "for sent_tag in sent_tagged_in_text:\n",
    "    # Find the entities for each sentence\n",
    "    NER_nltk_sent = []\n",
    "    ne_chunked_sent = check_entity_sent(sent_tag, \"\"\"\n",
    "                        NOUNS: {<N.*>{2,}}\n",
    "                        NOUN WITH ADJECTIVE: {<DT>?<JJ*><NN|NNS>}\n",
    "                            {<DT|PP\\$>?<JJ><NN>}\n",
    "                        PROPER NOUN: {<NNP*>+} \n",
    "                        \"\"\")\n",
    "    NER_nltk2.append(ne_chunked_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPER NOUN: \n",
      "  United - 51\n",
      "  Solskjaer - 47\n",
      "  Mourinho - 23\n",
      "  hydrogen - 21\n",
      "  deal - 21\n",
      "  Huawei - 15\n",
      "  job - 14\n",
      "  % - 13\n",
      "  MPs - 13\n",
      "  way - 12\n",
      "NOUNS: \n",
      "  Old Trafford - 11\n",
      "  Champions League - 9\n",
      "  Premier League - 8\n",
      "  Manchester United - 8\n",
      "  home draw - 6\n",
      "  Mrs May - 6\n",
      "  Ole Gunnar Solskjaer - 5\n",
      "  Orkney ’ - 4\n",
      "  % possession - 4\n",
      "  Louis van Gaal - 4\n",
      "NOUN WITH ADJECTIVE: \n",
      "  last year - 7\n",
      "  the first time - 7\n",
      "  clean energy - 6\n",
      "  ’ t - 5\n",
      "  fossil fuels - 4\n",
      "  the first leg - 4\n",
      "  the 21st manager - 3\n",
      "  final game - 3\n",
      "  young players - 3\n",
      "  surplus electricity - 2\n"
     ]
    }
   ],
   "source": [
    "cnt_NER2 = {}\n",
    "for text in NER_nltk2:\n",
    "    for sent in text:\n",
    "        for i in sent:\n",
    "            count_words_per_category(cnt_NER2, sent[i], i)\n",
    "print_best_n_for_each_category(cnt_NER2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find results in wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract first entity from text\n",
    "def extractFirstEntities(ne_chunked):\n",
    "    for entity in ne_chunked:\n",
    "        if isinstance(entity, nltk.tree.Tree):\n",
    "            text = \" \".join([word for word, tag in entity.leaves()])\n",
    "            ent = entity.label()\n",
    "            return text, ent\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_entity(tagged, pattern):\n",
    "    #print(tagged)\n",
    "    cp = nltk.RegexpParser(pattern)\n",
    "    return extractFirstEntities(cp.parse(tagged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_detection_from_wikipedia(data_dict):\n",
    "    result = []\n",
    "    cnt = 0\n",
    "    for cat in data_dict:\n",
    "        cnt += 1\n",
    "        for entity in data_dict[cat]:\n",
    "            try:\n",
    "                results = wikipedia.search(entity)\n",
    "                desc = \"\"\n",
    "                if len(results) > 0:\n",
    "                    desc = check_entity(nltk.pos_tag(nltk.word_tokenize(wikipedia.page(results[0]).summary)),\n",
    "                                       \"\"\"IS: {<VB.*>+<DT>+<JJ.*>*<NN.*>+}\"\"\")[0]\n",
    "                    desc = check_entity(nltk.pos_tag(nltk.word_tokenize(desc)),\n",
    "                                       \"\"\"IS: {<JJ.*>*<NN.*>+}\"\"\")[0]\n",
    "                if len(desc) == 0:\n",
    "                        desc = 'Thing'\n",
    "                result.append([entity, desc, cat, data_dict[cat][entity]])\n",
    "            except:\n",
    "                continue\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_some_from_wiki(data, n):\n",
    "    cnt = 0\n",
    "    for i in data:\n",
    "        if cnt == n:\n",
    "            break\n",
    "        cnt += 1\n",
    "        print(i[0], \"Description: \" + i[1], \"Entity: \" + i[2], \"Number in text: \" + str(i[3]), sep='\\n   ' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find wikipedia results in entity recognition by nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_anotated_nltk_entity = category_detection_from_wikipedia(cnt_NER)\n",
    "wiki_anotated_nltk_entity.sort(key=lambda x: x[3])\n",
    "wiki_anotated_nltk_entity = wiki_anotated_nltk_entity[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orkney\n",
      "   Description: archipelago\n",
      "   Entity: PERSON\n",
      "   Number in text: 150\n",
      "EMEC\n",
      "   Description: UKAS\n",
      "   Entity: ORGANIZATION\n",
      "   Number in text: 75\n",
      "UK\n",
      "   Description: sovereign country\n",
      "   Entity: ORGANIZATION\n",
      "   Number in text: 60\n",
      "Shapinsay\n",
      "   Description: eighth largest island\n",
      "   Entity: GPE\n",
      "   Number in text: 60\n",
      "Orkney\n",
      "   Description: archipelago\n",
      "   Entity: GPE\n",
      "   Number in text: 60\n",
      "Bews\n",
      "   Description: inherent limitations\n",
      "   Entity: PERSON\n",
      "   Number in text: 45\n",
      "Orcadians\n",
      "   Description: people\n",
      "   Entity: ORGANIZATION\n",
      "   Number in text: 30\n",
      "Orkney\n",
      "   Description: archipelago\n",
      "   Entity: ORGANIZATION\n",
      "   Number in text: 30\n",
      "Stromness\n",
      "   Description: second-most populous town\n",
      "   Entity: GPE\n",
      "   Number in text: 30\n",
      "Scotland\n",
      "   Description: country\n",
      "   Entity: GPE\n",
      "   Number in text: 30\n"
     ]
    }
   ],
   "source": [
    "print_some_from_wiki(wiki_anotated_nltk_entity, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find wikipedia results in entity recognition by my own patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del from entity take by my pattern entity with some count for get from wikipedia -> this take a long time\n",
    "size = 4\n",
    "delel = []\n",
    "for i in cnt_NER2:\n",
    "    for j in cnt_NER2[i]:\n",
    "        if cnt_NER2[i][j] < 3:\n",
    "            delel.append((i, j))\n",
    "for i in delel:\n",
    "    del cnt_NER2[i[0]][i[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_anotated_my_entity = category_detection_from_wikipedia(cnt_NER2)\n",
    "wiki_anotated_my_entity.sort(key=lambda x: x[3])\n",
    "wiki_anotated_my_entity = wiki_anotated_my_entity[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solskjaer\n",
      "   Description: Norwegian football manager\n",
      "   Entity: PROPER NOUN\n",
      "   Number in text: 47\n",
      "Mourinho\n",
      "   Description: Portuguese professional football coach\n",
      "   Entity: PROPER NOUN\n",
      "   Number in text: 23\n",
      "hydrogen\n",
      "   Description: chemical element\n",
      "   Entity: PROPER NOUN\n",
      "   Number in text: 21\n",
      "Huawei\n",
      "   Description: Chinese multinational telecommunications equipment\n",
      "   Entity: PROPER NOUN\n",
      "   Number in text: 15\n",
      "job\n",
      "   Description: person\n",
      "   Entity: PROPER NOUN\n",
      "   Number in text: 14\n",
      "%\n",
      "   Description: symbol\n",
      "   Entity: PROPER NOUN\n",
      "   Number in text: 13\n",
      "way\n",
      "   Description: eighth studio album\n",
      "   Entity: PROPER NOUN\n",
      "   Number in text: 12\n",
      "Old Trafford\n",
      "   Description: football stadium\n",
      "   Entity: NOUNS\n",
      "   Number in text: 11\n",
      "Dumbo\n",
      "   Description: mouse\n",
      "   Entity: PROPER NOUN\n",
      "   Number in text: 11\n",
      "UK\n",
      "   Description: sovereign country\n",
      "   Entity: PROPER NOUN\n",
      "   Number in text: 11\n"
     ]
    }
   ],
   "source": [
    "print_some_from_wiki(wiki_anotated_my_entity, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
